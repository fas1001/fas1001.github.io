---
title: "Cours 7 : Web scraping"
subtitle: "Introduction aux m√©gadonn√©es en sciences sociales"
author: Laurence-Olivier M. Foisy
institute: Universit√© de Montr√©al
lang: fr
from: markdown+emoji
format:
  revealjs:
    mermaid: 
      theme: default
    theme: [simple, custom.scss]
    slide-number: true
    logo: img/udem.png 
    footer: "[FAS1001](https://fas1001.com)"
    transition: slide
    transition-speed: fast
    code-fold: false
    code-overflow: wrap
    highlight-style: github
    embed-resources: true
---

# Retour sur le TP2 - Travail de Mi-Session

- Le but du TP2
- Difficile?
- Questions?

# Internet

## Qu'est-ce qu'Internet? {.smaller}

:::: {.columns}
::: {.column width="60%"}

### Un r√©seau de r√©seaux

- **Interconnexion globale** de r√©seaux informatiques
- **√âchange de donn√©es** via des protocoles standardis√©s
- Invent√© dans les ann√©es 1960s (ARPANET)
- Devenu public dans les ann√©es 1990s

### Ce n'est pas le Web

- Internet = l'infrastructure
- Web = un service parmi d'autres (email, FTP, etc.)
:::

::: {.column width="40%"}
![](img/arpanet.jpg){width="100%"}
<p style="font-size: 0.6em; text-align: center;">ARPANET en 1973</p>
:::
::::

## Architecture client-serveur {.smaller}

:::: {.columns}
::: {.column width="50%"}

**Principe fondamental**

- **Client** : demande des ressources
- **Serveur** : fournit des ressources

**Exemples**

- Client : navigateur web, application mobile
- Serveur : ordinateur h√©bergeant un site web

:::

::: {.column width="50%"}

![](https://upload.wikimedia.org/wikipedia/commons/thumb/c/c9/Client-server-model.svg/1200px-Client-server-model.svg.png){width="100%"}

:::
::::

## R√©alit√© physique d'Internet {.smaller}

:::: {.columns}
::: {.column width="60%"}
### Internet est tangible
- C√¢bles sous-marins traversant les oc√©ans
- Fibre optique, cuivre, satellites
- Routeurs et commutateurs
- Serveurs physiques dans des datacenters

### Le "cloud" n'existe pas
- "Le cloud, c'est juste l'ordinateur de quelqu'un d'autre"
- Les donn√©es sont stock√©es physiquement quelque part
:::

::: {.column width="40%"}
![](img/internet_map.jpg){width="100%"}
<p style="font-size: 0.6em; text-align: center;">Carte physique de l'internet</p>
:::
::::

## {background-image="img/internet_map.jpg"}

## Traceroute vers umontreal.ca

```bash
‚ùØ tcptraceroute umontreal.ca
Selected device wlp3s0, address 192.168.2.71, port 45431 for outgoing packets
Tracing the path to umontreal.ca (132.204.8.144) on TCP port 80 (http), 30 hops max
 1  192.168.2.1  5.618 ms  1.743 ms  2.499 ms
 2  10.11.16.41  3.860 ms  4.041 ms  3.737 ms
 3  * **
 4  64.230.36.102  6.344 ms  7.423 ms  9.020 ms
 5  64.230.91.65  5.214 ms  5.570 ms  6.500 ms
 6  192.77.55.233  6.890 ms  6.169 ms  5.786 ms
 7  imtrl-rq-ic-dmtrl-rq.risq.net (192.77.55.246)  5.837 ms  9.565 ms  7.724 ms
 8  umontreal2-contenu-dmtrl-um.risq.net (132.202.51.145)  32.002 ms  6.428 ms  7.800 ms
 9  * **
10  umontreal2-contenu-membre.risq.net (206.167.253.66)  33.855 ms  8.017 ms  6.263 ms
11  * **
12  * **
13  varnish.ti.umontreal.ca (132.204.8.144) [open]  10.542 ms  9.826 ms  13.408 ms
```

## Le chemin d'une requ√™te web vers UdeM {.smaller}

:::: {.columns}
::: {.column width="50%"}
1. **Votre ordinateur (192.168.2.71)**  
   Point de d√©part √† Qu√©bec

2. **Routeur r√©sidentiel (192.168.2.1)** 
   Votre passerelle vers Internet

3. **Routeur Bell Fibe (10.11.16.41)**  
   Entr√©e dans l'infrastructure de Bell Canada

4. **Dorsale Bell Canada (64.230.36.102)**
   Infrastructure principale de Bell - c√¢bles √† haute capacit√© 
   (jusqu'√† 100 Tbps sur certaines lignes)

5. **Point r√©gional Bell (64.230.91.65)**
   Transfert vers Montr√©al - connexion interurbaine
:::

::: {.column width="50%"}

6. **RISQ - Point d'entr√©e (192.77.55.233)**
   Entr√©e dans le R√©seau d'Informations Scientifiques du Qu√©bec

7. **RISQ - Interconnexion Montr√©al (192.77.55.246)**
   Routage interne du r√©seau acad√©mique qu√©b√©cois

8. **RISQ - Passerelle UdeM (132.202.51.145)**
   La porte d'entr√©e sp√©cifique √† l'Universit√© de Montr√©al

9. **UdeM - R√©seau membre (206.167.253.66)**
    Transfert au r√©seau interne de l'UdeM

10. **Serveur Varnish UdeM (132.204.8.144)**
    Destination finale: serveur de cache acc√©l√©rant la livraison du site
:::
::::

## Infrastructure physique

### Essayez ceci!
Tapez dans votre navigateur: **http://142.250.217.110**  

:::: {.columns}

::: {.column width="50%"}

- Ordinateurs: utilisent des nombres (IP)
- Humains: pr√©f√®rent les noms (google.com)
- DNS = l'annuaire qui fait le lien

:::

::: {.column width="50%"}

```{mermaid}
flowchart LR
    A[Ordinateur] -->|1. O√π est google.com?| B[DNS]
    B -->|2. IP: 142.250.217.110| A
    A -->|3. Connexion| C[Google]
```

:::
::::

## Empreinte physique des donn√©es {.smaller}

:::: {.columns}

::: {.column width="50%}

### O√π sont stock√©es vos donn√©es?
- Messages Messenger
- Photos Instagram
- Documents Google Drive
- Historique Netflix

### Redondance g√©ographique
- M√™mes donn√©es stock√©es dans plusieurs datacenters
- Protection contre pannes r√©gionales

:::

::: {.column width="50%"}

![](img/equinix.png)

![](img/aws.png)

:::

::::

## Quand vous allez sur un site

:::: {.columns}

::: {.column width="40%"}

1. JavaScript - Le comportement du site
2. HTML - La structure
3. CSS - Le style
4. API - Les donn√©es

:::

::: {.column width="60%"}

![](img/html_css_js.png)
:::

::::

# Web scraping

## C'est quoi le web scraping?

**Moissonnage / Aspiration / Grattage** de donn√©es sur le Web

:::: {.columns}
::: {.column width="45%"}

![](img/eureka_site.png)

:::
::: {.column width="10%"}

‚û°Ô∏è  

:::
::: {.column width="45%"}

![](img/eureka_df.png)

:::
::::

## 

![](img/time_series.png)

##

![](img/agreg.gif)

## R√©volution num√©rique

1. Acc√®s exhaustif √† d'immense corpus de documents
m√©dias, accords internationaux, discours et proc√©dures parlementaires, rapports annuels
d'entreprises, jurisprudences, etc.

2. La trace digitale des r√©seaux sociaux permet l'√©tude de nombreux ph√©nom√®nes sociaux

## √âtude de cas : Premiers ministres du Canada {.smaller}

::: {.callout-important}
### Question de recherche 
**Est-ce que se faire √©lire jeune est un avantage pour rester premier ministre longtemps?**

:::

Pour r√©pondre, nous devons recueillir:

- Dates de naissance des premiers ministres
- Leur √¢ge √† l'entr√©e en fonction
- Leur dur√©e au pouvoir
- Analyser la corr√©lation entre ces variables

## √Ä quel √¢ge Justin Trudeau est entr√© au pouvoir? {.smaller}

:::: {.columns}
::: {.column width="60%"}
### M√©thode manuelle traditionnelle
1. **Navigateur**: Ouvrir Google
2. **Recherche**: "Justin Trudeau date naissance" + "date entr√©e fonction"
3. **S√©lection**: Choisir une source fiable
4. **Information**: Extraire les dates
5. **Calcul**: D√©terminer l'√¢ge
:::

::: {.column width="40%"}
### R√©sultats
- N√© le: **25 d√©cembre 1971**
- Premier mandat: **4 novembre 2015**
- √Çge √† l'entr√©e: **43 ans**
:::
::::

## L'inefficacit√© de l'approche manuelle

::: {.callout-warning}
### R√©p√©ter pour CHAQUE premier ministre...

- **Justin Trudeau**: 1. Rechercher, 2. S√©lectionner, 3. Extraire, 4. Calculer, 5. Noter
- **Stephen Harper**: 1. Rechercher, 2. S√©lectionner, 3. Extraire, 4. Calculer, 5. Noter
- **Paul Martin**: 1. Rechercher, 2. S√©lectionner, 3. Extraire, 4. Calculer, 5. Noter
- **Jean Chr√©tien**: 1. Rechercher, 2. S√©lectionner, 3. Extraire, 4. Calculer, 5. Noter
- **Kim Campbell**: 1. Rechercher, 2. S√©lectionner, 3. Extraire, 4. Calculer, 5. Noter

**...et encore 18 autres premiers ministres!** üòì
:::

## Comment approcher le probl√®me?

:::: {.columns}
::: {.column width="25%"}
### 1. Naviguer
Identifier sources et structure des pages
:::

::: {.column width="25%"}
### 2. Aspirer
T√©l√©charger le contenu 
:::

::: {.column width="25%"}
### 3. Extraire
Isoler les donn√©es pertinentes
:::

::: {.column width="25%"}
### 4. Nettoyer
Structurer pour l'analyse
:::
::::

> Ces quatre √©tapes forment le processus standard du web scraping et peuvent √™tre automatis√©es avec R

## 1. Naviguer

Trouver les donn√©es voulues

- Identifier les sources potentielles (sites gouvernementaux)
- Explorer le site du Parlement du Canada
- D√©terminer si les donn√©es sont accessibles en HTML, JavaScript ou JSON
- Inspecter les requ√™tes r√©seau pour trouver des APIs cach√©es

## 1. Naviguer

![](img/1_search.png)

## 1. Naviguer

![](img/2_search.png)

## 1. Naviguer

![](img/3_search.png)

## 1. Naviguer

![](img/4_search.png)

## 1. Naviguer

### Comprendre la structure des pages web

- CTRL + U permet de voir le code source
- Click droit -> Inspecter <- permet de voir les √©l√©ments
- Onglet "R√©seau" pour observer les requ√™tes API

## 2. Aspirer

### Les formats de donn√©es

1. HTML : pour les pages web standards
2. JSON : pour les APIs (notre cas)
3. JavaScript : pour les pages dynamiques

## 2. Aspirer

```r
# URL of the page to scrape
url <- "https://lop.parl.ca/sites/ParlInfo/default/en_CA/People/primeMinisters"

# Read the webpage
page <- xml2::read_html(url)

elem <- rvest::html_table(page, "table.dx-datagrid-table")
```

### √áa ne fonctionne pas? Pourquoi?

‚û°Ô∏è  Retour sur le site web!

## 2. Aspirer

### Ctrl + U

Permet de voir que le tableau n'est pas visible dans la page

### Network tab

![](img/network_api.png)

##

![](img/full_json.png)

## 2. Aspirer

::: {.callout-tip}
Dans notre cas, nous avons d√©couvert une API du Parlement du Canada!
:::

```r
# Exemple de notre URL d'API pour les premiers ministres
api_url <- "https://lop.parl.ca/ParlinfoWebAPI/Person/SearchAndRefine?&refiners=5-1,&projectionId=5&callback=jQuery3600941572194102592_1741730322226&_=1741730322227"
```

::: {.callout-note}
Les API sont souvent masqu√©es mais peuvent √™tre d√©couvertes en inspectant les requ√™tes r√©seau!
:::

## 2. Aspirer - Anatomie d'un URL {.smaller}

::: {.callout-note appearance="minimal"}
`https://www.parlement.ca/premiers-ministres?periode=1950-2020&lang=fr`
:::

:::: {.columns}
::: {.column width="20%"}
**Protocole**
`https://`
*Communication s√©curis√©e*
:::

::: {.column width="30%"}
**Domaine**
`www.parlement.ca`
*Localisation du site*
:::

::: {.column width="25%"}
**Chemin**
`/premiers-ministres`
*Ressource sp√©cifique*
:::

::: {.column width="25%"}
**Param√®tres**
`?periode=1950-2020&lang=fr`
*Filtres et options*
:::
::::

> Comprendre ces composants permet d'identifier quelles parties de l'URL modifier pour acc√©der aux donn√©es souhait√©es lors du web scraping

## 2. Aspirer

### Utilisation de httr pour les requ√™tes API

```r
# Envoi de la requ√™te HTTP √† l'API
response <- httr::GET(api_url)

# Extraction du contenu textuel de la r√©ponse
content <- httr::content(response, "text", encoding = "UTF-8")
```

## 3. Extraire

### JSON vs HTML

Pour le HTML : `library(rvest)` et `library(xml2)`

Pour le JSON (notre cas) :
```r
# Nettoyage de la r√©ponse pour extraire uniquement la partie JSON
json_data <- stringr::str_extract(content, "\\[\\{.+\\}\\]")

# Conversion du JSON en objet R
pm_data <- jsonlite::fromJSON(json_data)
```

## 3. Extraire {.smaller}

### Structure JSON

`json$premiers_ministres$details$parti$nom`

```json
{
  "premiers_ministres": [
    {
      "nom": "Justin Trudeau",
      "details": {
        "naissance": "1971-12-25",
        "parti": {
          "nom": "Lib√©ral",
          "couleur": "rouge"
        }
      },
      "en_fonction": true
    },
    {
      "nom": "Stephen Harper",
      "details": {
        "naissance": "1959-04-30",
        "parti": {
          "nom": "Conservateur",
          "couleur": "bleu"
        }
      },
      "en_fonction": false
    }
  ],
  "derniere_mise_a_jour": "2023-09-15"
}
```

## 3. Extraire

### Traitement des donn√©es JSON

```r
# Cr√©ation d'un dataframe avec les informations principales
df_pm <- data.frame(
  name = paste(pm_data$UsedFirstName, pm_data$LastName),
  yob = pm_data$DateOfBirth,                  # Date de naissance
  Party = pm_data$PartyEn,                    # Parti politique
  occupation = pm_data$ProfessionsEn,         # Profession
  province = pm_data$ProvinceOfBirthEn,       # Province de naissance
  stringsAsFactors = FALSE
)
```

## 3. Extraire

### Naviguer dans les structures JSON complexes

```r
start_date <- c()
end_date <- c()
# Parcourir chaque premier ministre pour extraire les dates de mandat
for (i in 1:nrow(pm_data)) {
  # R√©cup√©rer tous les r√¥les politiques de la personne
  roles <- pm_data$Roles[[i]]
  
  # Parcourir chaque r√¥le pour trouver "Premier ministre"
  for (j in 1:nrow(roles)) {
    # V√©rifier si le titre inclut "Premier ministre"
    if ("Premier ministre" %in% roles$NameFr) {
      # Enregistrer les dates de d√©but et fin
      start_date[i] <- roles$StartDate[j]
      end_date[i] <- roles$EndDate[j]
    }
  }
}
```

## 4. Nettoyer

### Pr√©paration pour l'analyse

```r
# Conversion des dates en format Date de R
df_pm$start_date <- as.Date(substr(start_date, 1, 10))
df_pm$end_date <- as.Date(substr(end_date, 1, 10))
df_pm$yob <- as.Date(substr(df_pm$yob, 1, 10))

# Calcul de variables d'int√©r√™t
df_pm$duration <- as.numeric(difftime(df_pm$end_date, 
                                     df_pm$start_date, 
                                     units = "days"))/365.25

df_pm$age_at_start <- as.numeric(difftime(df_pm$start_date, 
                                         df_pm$yob, 
                                         units = "days"))/365.25
```

## 5. Visualiser et analyser

### Cr√©ation d'un graphique

```r
# Visualisation avec ggplot2
ggplot2::ggplot(df_pm, ggplot2::aes(x = age_at_start)) +
  ggplot2::geom_histogram(binwidth = 5, fill = "#CF0E20") +
  ggplot2::labs(
    title = "√Çge des premiers ministres canadiens √† leur entr√©e en fonction",
    x = "√Çge au d√©but du mandat (ann√©es)",
    y = "Fr√©quence"
  )
```

## 5. Visualiser et analyser

![](img/age_pm.png)

## 5. Visualiser et analyser

### Analyse statistique

```r
# R√©gression lin√©aire: dur√©e ~ √¢ge au d√©but
model <- lm(duration ~ age_at_start, data = df_pm)
summary(model)
```
```txt
r$> summary(m)

Call:
lm(formula = duration ~ age_at_start, data = df_pm)

Residuals:
    Min      1Q  Median      3Q     Max 
-5.6546 -3.6723 -0.0936  3.4233  9.6509 

Coefficients:
             Estimate Std. Error t value Pr(>|t|)
(Intercept)   8.40307    5.52676   1.520    0.143
age_at_start -0.05162    0.09679  -0.533    0.599

Residual standard error: 4.515 on 21 degrees of freedom
Multiple R-squared:  0.01336,   Adjusted R-squared:  -0.03362 
F-statistic: 0.2845 on 1 and 21 DF,  p-value: 0.5994
```

::: {.callout-important}
Est-ce que les plus jeunes premiers ministres ont tendance √† rester plus longtemps en poste? NON!
:::

# Exemples concrets

## Agr√©gateur de sondages √©lectorales

### Les trois approches principales du Web Scraping {.smaller}

1. Extraction des tableaux HTML/CSS (`rvest`)
2. T√©l√©chargement direct de fichiers (CSV, Excel)
3. Connexion aux API REST (`httr`, `jsonlite`)

## Notre Projet : Agr√©gateur de Sondages √âlectoraux {.smaller}

### Trois sources de donn√©es √©lectorales canadiennes:

1. **338Canada**: Tables HTML avec projections √©lectorales
2. **Poliwave**: T√©l√©chargement direct de fichiers Excel
3. **The Signal**: API REST (endpoints JSON)

### Structure du pipeline de donn√©es:
- Extraction ‚Üí Stockage brut ‚Üí Transformation ‚Üí Visualisation

## Agr√©gateur de Sondages √âlectoraux - Objectif {.smaller}

Pour chaque m√©thode de scraping, nous verrons:

1. Les packages et fonctions cl√©s
2. Le code √©tape par √©tape 
3. Les avantages et inconv√©nients

# 1. Web Scraping avec HTML/CSS {background-color="#4A235A" .center}

## Canada 338 {.smaller}

### Le site Canada 338

- Pr√©sente des projections √©lectorales par circonscription
- Les donn√©es sont organis√©es dans un tableau HTML
- Identifi√© par l'ID CSS `#myTable`

### Notre approche

1. Charger la page principale
2. Extraire le tableau HTML des projections
3. Nettoyer et structurer les donn√©es
4. Sauvegarder en format RDS pour analyse ult√©rieure

## Canada 338

```r
# Chargement des packages n√©cessaires
library(rvest)     # Pour le scraping HTML
library(dplyr)     # Pour la manipulation de donn√©es
library(stringr)   # Pour la manipulation de texte

# 1. URL de la page √† scraper
url <- "https://338canada.com/districts.htm"

# 2. Lecture de la page web
html_content <- read_html(url)

# 3. Extraction du tableau HTML
table_data <- html_content %>%
  html_node("#myTable") %>%     # S√©lectionne le tableau par son ID CSS
  html_table(fill = TRUE)       # Convertit en dataframe

```

## Canada 338 {.smaller}

```r
# 4. Extraction des informations utiles
# 4.1 Extraire l'ID de circonscription (format 12345)
table_data$riding_id <- str_extract(table_data$electoral_district, "\\d{5}")

# 4.2 Extraire le nom propre de la circonscription
table_data$district_name <- str_replace_all(table_data$electoral_district, "\\d{5} ", "")

# 4.3 Extraire le parti projet√© gagnant (LPC, CPC, NDP, BQ, GPC)
table_data$projected_party <- str_extract(table_data$latest_projection, "(LPC|CPC|NDP|BQ|GPC)")

# 4.4 Extraire le statut de la projection (safe, likely, leaning)
table_data$projection_status <- str_replace_all(table_data$latest_projection, "(LPC|CPC|NDP|BQ|GPC) ", "")

# 5. Cr√©ation d'un dataframe propre et sauvegarde
clean_data <- table_data %>%
  select(riding_id, district_name, projected_party, projection_status) %>%
  filter(!is.na(riding_id))  # Supprime les lignes sans ID (ent√™tes)

# 6. Ajout de la date du scraping et sauvegarde
clean_data$scrape_date <- Sys.Date()
saveRDS(clean_data, "data/lake/can338/338_canada_projection.rds")
```

## 338Canada - Comment trouver les bons s√©lecteurs CSS? {.smaller}

### Outils du navigateur
1. Clic droit sur l'√©l√©ment ‚Üí Inspecter
2. Explorer l'arbre HTML
3. Identifier les balises et attributs uniques

### Strat√©gies pour les tableaux
- Trouver l'ID du tableau: `#myTable`
- S√©lectionner par position: `table:nth-child(2)`
- Par classe: `.data-table`
- Combinaison: `div.content table`

### La fonction html_table()
- Convertit un tableau HTML directement en dataframe
- Option `fill = TRUE` pour les tableaux irr√©guliers
- Pr√©serve la structure des colonnes

## 338Canada - Avantages et inconv√©nients {.smaller}

### Inconv√©nients
- D√©pendance √† la structure HTML du site
- Sensible aux modifications de mise en page
- Traitement suppl√©mentaire souvent n√©cessaire
- Limitations pour les tableaux dynamiques (JavaScript)

### Consid√©rations
- V√©rifier la stabilit√© du tableau dans le temps
- Pr√©voir un plan B si la structure change
- Ajouter la date d'extraction pour tra√ßabilit√©
- Impl√©menter des v√©rifications de validit√©

# 2. T√©l√©chargement direct de fichiers {background-color="#0B5345" .center}

## Poliwave - Notre objectif {.smaller}

### Le site Poliwave
- Publie des projections √©lectorales pour le Canada
- Fournit un fichier Excel avec les donn√©es par circonscription
- URL directe: https://www.poliwave.com/Canada/Federal/Data/CAtable.xlsx

### Notre approche
Simplissime: T√©l√©charger directement le fichier Excel et le sauvegarder!

## Poliwave {.smaller}

```r
# Chargement du package pour Excel
library(readxl)

# 1. Cr√©ation d'un fichier temporaire avec extension .xlsx
temp_file <- tempfile(fileext = ".xlsx")

# 2. T√©l√©chargement du fichier
download.file(
  "https://www.poliwave.com/Canada/Federal/Data/CAtable.xlsx", 
  temp_file, 
  mode = "wb"  # Mode binaire important pour Excel
)

# 3. Lecture du fichier Excel
poliwave_ridings <- readxl::read_xlsx(temp_file)

# 4. Sauvegarde en format RDS
saveRDS(
  poliwave_ridings,
  "data/lake/poliwave/poliwave_table.rds"
)
```

## Poliwave - Avantages et inconv√©nients {.smaller}

:::: {.columns}
::: {.column width="50%"}
### Avantages
- Aucun parsing ou nettoyage n√©cessaire
- Tr√®s robuste aux changements du site
- Les donn√©es sont d√©j√† structur√©es
- Performance optimale (t√©l√©chargement direct)

### Cas d'usage id√©aux
- Sites qui proposent des exports directs
- Besoin des donn√©es compl√®tes
- Absence de filtrage n√©cessaire
- Mises √† jour r√©guli√®res des fichiers
:::

::: {.column width="50%"}
### Inconv√©nients
- Limit√© aux donn√©es explicitement partag√©es
- Pas de personnalisation possible
- Format impos√© par la source
- D√©pendance aux modifications d'URL
- Fichiers volumineux potentiels

### Consid√©rations
- V√©rifier r√©guli√®rement que l'URL est valide
- S'assurer que le format de fichier reste stable
- Pr√©voir une gestion des erreurs de t√©l√©chargement
- Documenter la structure attendue du fichier
:::
::::

# 3. API REST avec httr & jsonlite {background-color="#17202A" .center}

## API REST - Les Packages {.smaller}

| Package | Fonction | Description |
|---------|----------|-------------|
| `httr` | `GET()` | Effectue une requ√™te HTTP GET √† une URL |
| `httr` | `content()` | Extrait le contenu d'une r√©ponse HTTP |
| `jsonlite` | `fromJSON()` | Parse une cha√Æne JSON en objet R |

## The Signal - Notre objectif {.smaller}

### Le site The Signal
- Publie des projections √©lectorales d√©velopp√©es par CBC/Radio-Canada
- API cach√©e pour acc√©der aux donn√©es
- Structure des donn√©es en format JSON

### Notre approche
1. D√©finir l'URL de l'API
2. Effectuer une requ√™te GET √† l'endpoint
3. Parser la r√©ponse JSON en dataframe
5. Sauvegarder les r√©sultats


## The Signal {.smaller}

```r
# Chargement des packages
library(dplyr)    # Pour la manipulation de donn√©es
library(httr)     # Pour les requ√™tes HTTP
library(jsonlite) # Pour le parsing JSON

# 1. Configuration de base
# URL de l'API pour les projections √©lectorales
api <- "https://thesignal-api.voxpoplabs.com/api/en/canada2025/riding_projections"

# 4. Ex√©cution de la requ√™te API
response <- httr::GET(api)
```

## The Signal {.smaller}

```r
# 5. Extraction du texte JSON
json_text <- httr::content(response, as = "text", encoding = "UTF-8")

# 6. Conversion du JSON en dataframe
riding_data <- jsonlite::fromJSON(json_text)

# 9. Sauvegarde des r√©sultats
saveRDS(riding_data, "data/lake/signal_riding_projections.rds")
```

## Structure de r√©ponse JSON {.smaller}

```json
{
  "ridings": [
    {
      "riding_name": "Beauce",
      "region": "QC",
      "confidence": "Likely",
      "prediction": "CON",
      "date_updated": "2023-11-15"
    },
    {
      "riding_name": "Toronto Centre",
      "region": "ON",
      "confidence": "Safe",
      "prediction": "LIB",
      "date_updated": "2023-11-15"
    },
    {
      "riding_name": "Vancouver Centre",
      "region": "BC",
      "confidence": "Likely",
      "prediction": "LIB",
      "date_updated": "2023-11-15"
    }
  ],
  "meta": {
    "last_updated": "2023-11-15T12:00:00Z",
    "version": "1.0"
  }
}
```

## The Signal - Avantages et inconv√©nients {.smaller}

:::: {.columns}
::: {.column width="50%"}
### Avantages
- Facile √† extraire
- Possibilit√© de requ√™tes param√©tr√©es
- Extraction cibl√©e (√©conomie de donn√©es)

### Cas d'usage id√©aux
- Sites avec API publique
- Besoins de donn√©es sp√©cifiques
- Grande volum√©trie
- Besoin de robustesse et stabilit√©
- Requ√™tes param√©tr√©es ou filtr√©es
:::

::: {.column width="50%"}
### Inconv√©nients
- Peut n√©cessiter une authentification
- API parfois limit√©es ou payantes
- Quotas de requ√™tes possibles
- Peut n√©cessiter plus de code initial
- D√©pendance aux changements d'API

### Consid√©rations
- Lire la documentation de l'API
- V√©rifier les conditions d'utilisation
- Respecter les limites de taux
- G√©rer l'authentification si n√©cessaire
- S'adapter aux versions d'API
:::
::::

## Quand utiliser quelle approche? {.smaller}

:::: {.columns}
::: {.column width="33%"}
### HTML/CSS
- Pas d'API disponible
- Pas de donn√©es t√©l√©chargeables
- Besoin de donn√©es sp√©cifiques
- Donn√©es dans des tableaux HTML
- Besoin d'extraction automatis√©e

**Packages**: rvest, xml2
:::

::: {.column width="33%"}
### T√©l√©chargement direct
- Donn√©es d√©j√† agr√©g√©es
- Formats structur√©s disponibles
- Besoin du jeu complet
- Pas de param√©trage n√©cessaire
- Solution ultra-simple requise

**Packages**: utils, readxl, readr
:::

::: {.column width="33%"}
### API REST
- API officielle disponible
- Besoin de donn√©es sp√©cifiques
- Param√©trage des requ√™tes
- Volume de donn√©es important
- Besoin de robustesse

**Packages**: httr, jsonlite
:::
::::

## Robots.txt - Qu'est-ce que c'est? {.smaller}

:::: {.columns}
::: {.column width="65%"}
- Fichier texte plac√© √† la racine d'un site web
- Indique aux robots d'indexation et scrapers quelles parties du site:
  - **Peuvent** √™tre visit√©es et index√©es
  - **Ne doivent pas** √™tre visit√©es
- Standard de facto depuis 1994
- Respect√© par les moteurs de recherche et autres robots "√©thiques"
:::

::: {.column width="35%"}
```
User-agent: *
Disallow: /private/
Disallow: /admin/
Allow: /public/

User-agent: Googlebot
Allow: /
```
:::
::::

> Un guide pour les robots, mais pas une barri√®re de s√©curit√©
```

```
## Pourquoi respecter robots.txt? {.smaller}

::: {.callout-important}
### √âthique du web scraping
- **Respect des ressources** du serveur
- **Politesse num√©rique** envers les propri√©taires du site
- **Conformit√© l√©gale** (selon les juridictions)
:::

:::: {.columns}
::: {.column width="50%"}
### Cons√©quences possibles
- Bannissement de votre adresse IP
- Surcharge des serveurs
- Probl√®mes l√©gaux potentiels
:::

::: {.column width="50%"}
### Alternatives l√©gitimes
- Utiliser les API officielles quand disponibles
- Limiter la fr√©quence des requ√™tes
- Contacter le propri√©taire du site
:::
::::
```

```
## V√©rifier robots.txt en R {.smaller}

::: {.callout-note}
### La r√®gle g√©n√©rale: toujours v√©rifier avant de scraper!
:::

```r
# Installer le package si n√©cessaire
# install.packages("robotstxt")
library(robotstxt)

# V√©rifier si le scraping est autoris√©
paths_allowed("https://lop.parl.ca/sites/ParlInfo/default/en_CA/People/primeMinisters")
#> [1] TRUE

paths_allowed("https://lop.parl.ca/ParlinfoWebAPI/Person/SearchAndRefine?&refiners=5-1,&projectionId=5&callback=jQuery360032980273762339296_1741814357460&_=1741814357461")
#> [1] TRUE

# Examiner le fichier robots.txt complet
robotstxt::get_robotstxt("https://lop.parl.ca")
```

> Le package `robotstxt` facilite la v√©rification avant le web scraping

## robots.txt

```txt
r$> robotstxt::get_robotstxt("https://lop.parl.ca")
[robots.txt]
--------------------------------------

User-agent: *
Disallow: /staticfiles/PublicWebsite/Home/About/CoporateDocuments/PDFExc
ludedInRobotsTXT
Disallow: /content/lop/Quorum
Disallow: /Content/LOP/Quorum
Disallow: /Content/LOP/quorum
Disallow: /Content/lop/quorum
Disallow: /content/lop/quorum
Disallow: /content/LOP/Quorum
Disallow: /sites/PublicWebsite/default/en_CA/NotFound
Disallow: /sites/PublicWebsite/default/fr_CA/NotFound
Disallow: /sites/ParlInfo/default/en_CA/SiteInformation/parlinfoMoved
Disallow: /sites/ParlInfo/default/fr_CA/InformationSite/parlinfoDemenage
Allow: /sites/ParlInfo/default/en_CA/People/Profile?*
Disallow: /sites/ParlInfo/default/en_CA/People/Profile
Allow: /sites/ParlInfo/default/fr_CA/Personnes/Profil?*
Allow: /sites/ParlInfo/default/fr_CA/Personnes/Profile?*
Disallow: /sites/ParlInfo/default/fr_CA/Personnes/Profil
Disallow: /sites/ParlInfo/default/fr_CA/Personnes/Profile
Allow: /sites/ParlInfo/default/en_CA/Federal/areasResponsibility/profile
?*
Disallow: /sites/ParlInfo/default/en_CA/Federal/areasResponsibility/prof
ile
Allow: /sites/ParlInfo/default/fr_CA/Federal/domainesResponsabilites/pro
fil?*
Disallow: /sites/ParlInfo/default/fr_CA/Federal/domainesResponsabilites/
profil
Allow: /sites/ParlInfo/default/en_CA/Parties/Profile?*
Disallow: /sites/ParlInfo/default/en_CA/Parties/Profile
Allow: /sites/ParlInfo/default/fr_CA/Partis/Profil?*
Disallow: /sites/ParlInfo/default/fr_CA/Partis/Profil
Allow: /sites/ParlInfo/default/en_CA/ElectionsRidings/Ridings/Profile?*
Disallow: /sites/ParlInfo/default/en_CA/ElectionsRidings/Ridings/Profile
Allow: /sites/ParlInfo/default/fr_CA/ElectionsCirconscriptions/Circonscr
iptions/Profil?*
Disallow: /sites/ParlInfo/default/fr_CA/ElectionsCirconscriptions/Circon
scriptions/Profil
Allow: /sites/ParlInfo/default/en_CA/ElectionsRidings/Elections/Profile?
*
Disallow: /sites/ParlInfo/default/en_CA/ElectionsRidings/Elections/Profi
le
Allow: /sites/ParlInfo/default/fr_CA/ElectionsCirconscriptions/Elections
/Profil?*
Disallow: /sites/ParlInfo/default/fr_CA/ElectionsCirconscriptions/Electi
ons/Profil
Disallow: /sites/PublicWebsite/default/fr_CA/Staging
Disallow: /sites/PublicWebsite/default/en_CA/Staging
Disallow: /sites/PublicWebsite/default/fr_CA/merc
Disallow: /sites/PublicWebsite/default/en_CA/merc
Sitemap: https://bdp.parl.ca/rss/public/fr/sitemapxml
Sitemap: https://lop.parl.ca/rss/public/en/sitemapxml
```

## Lire et interpr√©ter robots.txt {.smaller}

:::: {.columns}
::: {.column width="50%"}
### Syntaxe courante

- `User-agent: *` - S'applique √† tous les robots
- `User-agent: Googlebot` - Sp√©cifique √† Google
- `Disallow: /dossier/` - Interdit l'acc√®s
- `Allow: /dossier/` - Autorise explicitement l'acc√®s
- `Crawl-delay: 10` - Attendre 10 secondes entre requ√™tes
:::

::: {.column width="50%"}
### Limitations

- Bas√© sur la confiance, pas une protection technique
- Interpr√©tation parfois ambigu√´
- Certains sites utilisent des r√®gles complexes
- Pas de norme officielle pour "User-agent: R-Scraper"
:::
::::

> Toujours associer la v√©rification de robots.txt √† d'autres bonnes pratiques de scraping (d√©lais, user-agent explicite, etc.)
```

```
## Bonnes pratiques au-del√† de robots.txt {.smaller}

:::: {.columns}
::: {.column width="50%"}
### Identifiez-vous
```r
session <- polite::bow(
  url = "https://example.com",
  user_agent = "FAS1001 Educational Scraper"
)
```
:::

::: {.column width="50%"}
### Respectez les d√©lais
```r
# Attendre entre les requ√™tes
Sys.sleep(2)  

# Ou utilisez polite
polite::scrape(session)
```
:::

::::

::: {.callout-tip}
Le package `polite` int√®gre la v√©rification de robots.txt et d'autres bonnes pratiques comme les d√©lais entre requ√™tes.
:::

## Pour aller plus loin {.smaller}

- `RSelenium`: Pour sites dynamiques (JavaScript)

### Ressources
- [Web Scraping with R (Hadley Wickham)](https://rvest.tidyverse.org/articles/rvest.html)
- [httr Quickstart](https://httr.r-lib.org/articles/quickstart.html)
- [APIs avec R (rOpenSci)](https://www.ropensci.org/)
- [Le web scraping en sciences sociales (ATILF)](https://halshs.archives-ouvertes.fr/halshs-02960306)
- [Ethical Web Scraping](https://towardsdatascience.com/ethics-in-web-scraping-b96b18136f01)

## Trucs et astuces {.smaller}

1. Utiliser les outils de d√©veloppement de votre navigateur
2. Soyez conscient des implications √©thiques
  - robots.txt
  - Sys.sleep(1)
3. Les tables HTML peuvent √™tre import√©es directement dans R
4. Utiliser les regex
5. Peaufiner vos comp√©tences en CSS
6. Utiliser les API officielles si possible
7. Des sites peuvent √™tre visit√©s avec la wayback machine
8. R√©v√©lez les API cach√©es dans les sites webs
9. Utiliser Selenium : Pour les utilisateurs avanc√©s


