---
title: "Cours 6 : L'analyse textuelle"
subtitle: "Introduction aux mégadonnées en sciences sociales"
author: Laurence-Olivier M. Foisy
institute: Université de Montréal
lang: fr
from: markdown+emoji
format:
  revealjs:
    mermaid: 
      theme: dark
    theme: simple
    logo: https://upload.wikimedia.org/wikipedia/en/thumb/4/4b/Universite_de_Montreal_logo.svg/1920px-Universite_de_Montreal_logo.svg.png
    footer: "[FAS1001](https://fas1001.com)"
    transition: slide
    transition-speed: fast
    code-fold: false
    code-overflow: wrap
bibliography: bibliography.bib
---

# Retour sur le TP1

- Chemins d'accès relatifs
- `../../results/graphs/graph.png`
- `.RProj`
- Le titre de votre document
- Les références .bib
- Les citations \@auteurdate et [\@auteurdate]
- Comment retourner sur GitHub pour voir l'ancienne version de votre TP1

## TP2 - Travail de mi-session {.smaller}

:::: {.columns}

::: {.column width="45%"}

1. À remettre le 12 mars 2025 avant minuit
2. 20% de la note finale
3. Préparation de l'environnement de travail
4. Création de l'arborescence
5. Préparation des données
6. Nettoyage de deux variables

:::
::: {.column width="10%"}

:::

::: {.column width="45%"}

7. Une visualisation
8. Un document quarto
9. Une question de recherche
10. Courte revue de littérature
11. Source et structure des données
12. Planification méthodologique

:::

:::::

## {transition="none"}

![](img/orga.jpg)

## {transition="none"}

![](img/orga_3d.jpg)

## Gabarit suggéré {.smaller}

1. Introduction
2. Question de recherche
3. Revue de littérature 
4. Hypothèses
5. Méthodologie
6. Résultats
7. Discussion
8. Conclusion

# Introduction à l'analyse textuelle

##

![](img/structure_cours.png)

## Pourquoi analyser du texte?

### Les données sont partout :earth_americas: :earth_africa: :earth_asia:

- Données textuelles disponibles en grande quantité
- Méthodes de recherche de plus en plus accessibles
- Idée(s) sur différentes sources de données textuelles? 

## {background-image="img/newspapers.webp" background-size="cover"}

## {background-image="img/assnat.avif" background-size="cover"}

## L'explosion des données textuelles

- Médias sociaux
- Articles de presse
  - [Eureka et Factiva](https://bib.umontreal.ca/guides/types-documents/journaux)
- Documents légaux
- Sondages (questions ouvertes)
- Courriels
- Messages instantanés
- Beaucoup plus

## Que peut-on faire avec le texte?

1. Analyse de sentiment 
2. Classification de documents 
3. Extraction de thèmes 
4. Résumé automatique 
5. Analyse de discours 

## Différentes méthodes d'analyse textuelle

![](img/schema_analyse_textuelle.png)


## De texte à données

```{mermaid}
%%{init: {"theme": "base", "themeVariables": {"primaryColor": "#f4f4f4", "textColor": "#333", "lineColor": "#666"}, "flowchart": {"nodeSpacing": 200, "rankSpacing": 150, "diagramPadding": 50, "htmlLabels": true}}}%%
graph LR
    A[Texte brut] --> B[Nettoyage]
    B --> C[Tokenisation]
    C --> D[Analyse]
    D --> E[Visualisation]
```

# Introduction aux expressions régulières

## Qu'est-ce qu'on essaie de faire?

- Trouver des patterns dans du texte
- Exemple : Trouver tous les numéros de téléphone dans un document
- Exemple : Extraire tous les courriels d'un texte

## Un exemple concret

Imaginons que vous ayez ce texte:

```text
Contact: Marie (514-555-1234) 
Courriel: marie@udem.ca
Contact: Pierre (438-555-5678)
Courriel: pierre@udem.ca
```

Comment trouver automatiquement tous les numéros de téléphone?

## La solution "manuelle"

1. Chercher des chiffres
2. Regroupés par 3 ou 4
3. Séparés par des tirets
4. Commençant par 514 ou 438

## La solution avec regex

```r
# Un pattern qui trouve les numéros de téléphone
"\\d{3}-\\d{3}-\\d{4}"
```

## Décomposons le pattern

- `\d` : un chiffre (0-9)
- `{3}` : exactement 3 fois
- `-` : un tiret
- Donc `\d{3}-\d{3}-\d{4}` trouve: "514-555-1234"

## Pourquoi c'est utile?

- Automatise la recherche de patterns
- Fonctionne sur n'importe quelle quantité de texte
- Plus rapide et fiable que la recherche manuelle
- Essentiel pour le nettoyage de données textuelles

## Utilisation dans R

### Le package stringr

```r
# Installation si nécessaire
install.packages("stringr")

# Chargement
library(stringr)
```

- Fait partie du tidyverse
- Fonctions simples et cohérentes
- Documentation claire avec de nombreux exemples

## Fonctions principales de stringr

```r
# Détecter un pattern
str_detect(string, pattern)

# Extraire un pattern
str_extract(string, pattern)

# Remplacer un pattern
str_replace(string, pattern, replacement)

# Séparer selon un pattern
str_split(string, pattern)
```

## Exemple pratique

```r
library(stringr)

# Notre texte
text <- "Contact: Marie (514-555-1234), Pierre (438-555-5678)"

# Trouver tous les numéros
numeros <- str_extract_all(text, "\\d{3}-\\d{3}-\\d{4}")

# Résultat
print(numeros)
# [[1]]
# [1] "514-555-1234" "438-555-5678"
```

## Les briques de base des regex{.smaller}

### Rechercher des chiffres avec `\d`
```r
texte <- "Je suis né en 1990 et j'ai 33 ans"
str_extract_all(texte, "\\d+")
# Résultat: [1] "1990" "33"
```

### Rechercher des lettres avec `[A-Z]` et `[a-z]`
```r
texte <- "QUÉBEC est une ville du Canada"
# Trouver les mots en majuscules
str_extract(texte, "[A-Z]+")
# Résultat: "QUÉBEC"
```

### Rechercher des espaces avec `\s`
```r
texte <- "mot     clé"
# Remplacer les espaces multiples par un seul espace
str_replace_all(texte, "\\s+", " ")
# Résultat: "mot clé"
```

## Les quantificateurs : Combien de fois?{.smaller}

### Le `+` : "un ou plusieurs"
```r
# Trouver les nombres (un ou plusieurs chiffres)
texte <- "J'ai 1 chat et 22 poissons"
str_extract_all(texte, "\\d+")
# Résultat: [1] "1" "22"
```

### Le `*` : "zéro ou plusieurs"
```r
# Trouver les mots avec ou sans 's' à la fin
texte <- "chat chats chien chiens"
str_extract_all(texte, "chat[s]*")
# Résultat: [1] "chat" "chats"
```

### Le `?` : "optionnel (zéro ou un)"
```r
# Trouver 'behaviour' ou 'behavior'
texte <- "behaviour and behavior"
str_extract_all(texte, "behaviou?r")
# Résultat: [1] "behaviour" "behavior"
```

## Exemples pratiques pour les sciences sociales{.smaller}

### Extraire des codes postaux canadiens
```r
adresse <- "Mon adresse est H2X 1Y6"
str_extract(adresse, "[A-Z]\\d[A-Z]\\s?\\d[A-Z]\\d")
# Résultat: "H2X 1Y6"
```

### Extraire des identifiants Twitter
```r
tweet <- "Suivez-moi @MonProfR et @UdeM"
str_extract_all(tweet, "@\\w+")
# Résultat: [1] "@MonProfR" "@UdeM"
```

### Identifier des URLs dans un texte
```r
texte <- "Visitez https://www.umontreal.ca pour plus d'infos"
str_extract(texte, "https?://[\\w\\.]+\\.\\w+")
# Résultat: [1] "https://www.umontreal.ca"
```

## Comment utiliser les regex en R?

:::: {.columns}

::: {.column width="40%"}

1. Les numéros de téléphone?
2. Les adresses courriel?
3. Les pourcentages?

**L'intelligence artificielle :wink:**

:::

::: {.column width="65%"}

![](img/regex_claude.png){.absolute top=100 left=600 width=50%}
:::

::::


# Analyse de sentiment

## La ligne rouge {transition="none"}


::: {.columns}

::: {.column width="60%"}

> Super good kebab! The portions are generous, the prices are really reasonable, and the quality is there. Tasty meat, fresh bread, and everything is well seasoned. An excellent address for a meal that is good without breaking the bank. I recommend!

:::

::: {.column width="40%"}

- Sentiment: Positif
- Thèmes: Nourriture, Prix
- Note: 5/5

:::

::::


## Étape 1: Création du datafram {transition="none"}
e
```r
# Créer un data.frame avec notre review
review <- data.frame(
  restaurant = "La ligne rouge",
  text = "Super good kebab! The portions are generous, the prices are really reasonable, and the quality is there. Tasty meat, fresh bread, and everything is well seasoned. An excellent address for a meal that is good without breaking the bank. I recommend!",
  stringsAsFactors = FALSE
)
```

### Donne un dataframe comme ceci


| restaurant      | text                    |
|-----------------|-------------------------|
| La ligne rouge  | Super good kebab! [...] |



## Étape 2: Nettoyage de base {transition="none"}
```r
# Nettoyage avec stringr
review_clean <- review
review_clean$text <- stringr::str_to_lower(review_clean$text)                 # Minuscules
review_clean$text <- stringr::str_remove_all(review_clean$text, "!")          # Exclamations
review_clean$text <- stringr::str_remove_all(review_clean$text, "\\.")        # Points
review_clean$text <- stringr::str_remove_all(review_clean$text, ",")          # Virgules
```

::: {.columns}
::: {.column width="100%"}
> <span style="color:#880808">S</span>uper good kebab<span style="color:#880808">!</span> <span style="color:#880808">T</span>he portions are generous<span style="color:#880808">,</span> the prices are really reasonable<span style="color:#880808">,</span> and the quality is there<span style="color:#880808">.</span> <span style="color:#880808">T</span>asty meat<span style="color:#880808">,</span> fresh bread<span style="color:#880808">,</span> and everything is well seasoned<span style="color:#880808">.</span> <span style="color:#880808">A</span>n excellent address for a meal that is good without breaking the bank<span style="color:#880808">.</span> <span style="color:#880808">I</span> recommend<span style="color:#880808">!</span>
:::
:::

## Étape 2: Nettoyage de base {transition="none"}
```r
# Nettoyage avec stringr
review_clean <- review
review_clean$text <- stringr::str_to_lower(review_clean$text)                 # Minuscules
review_clean$text <- stringr::str_remove_all(review_clean$text, "!")          # Exclamations
review_clean$text <- stringr::str_remove_all(review_clean$text, "\\.")        # Points
review_clean$text <- stringr::str_remove_all(review_clean$text, ",")          # Virgules
```

::: {.columns}
::: {.column width="100%"}
> super good kebab the portions are generous the prices are really reasonable and the quality is there tasty meat fresh bread and everything is well seasoned an excellent address for a meal that is good without breaking the bank i recommend
:::
:::

## Étape 3: Tokenisation {transition="none"}


:::: {.columns}

::: {.column width="60%"}

```r
tokens <- tidytext::unnest_tokens(
  review_clean,
  output = word,
  input = text
)

```
:::

::: {.column width="40%"}


```
r$> head(tokens, 10)
       restaurant     word
1  La ligne rouge    super
2  La ligne rouge     good
3  La ligne rouge    kebab
4  La ligne rouge      the
5  La ligne rouge portions
6  La ligne rouge      are
7  La ligne rouge generous
8  La ligne rouge      the
9  La ligne rouge   prices
10 La ligne rouge      are
```

:::

::::

## Étape 4: Retrait des mots vides {transition="none"}


:::: {.columns}

::: {.column width="60%"}

```r
# Obtenir les stop words
stop_words <- tidytext::get_stopwords(language = "en")

# Retirer les stop words avec dplyr anti_join
tokens_clean <- dplyr::anti_join(
  tokens, 
  stop_words,
  by = "word"
)

```

:::

::: {.column width="40%"}

```
r$> head(tokens_clean, 10)
       restaurant       word
1  La ligne rouge      super
2  La ligne rouge       good
3  La ligne rouge      kebab
4  La ligne rouge   portions
5  La ligne rouge   generous
6  La ligne rouge     prices
7  La ligne rouge     really
8  La ligne rouge reasonable
9  La ligne rouge    quality
10 La ligne rouge      tasty
```

:::

::::

## Étape 5: Analyse de sentiment (AFINN) {transition="none"}


::: {.columns}

::: {.column width="70%"}

```r
# Obtenir le lexique AFINN
afinn <- tidytext::get_sentiments("afinn")

# Joindre avec nos tokens
sentiment_scores <- dplyr::inner_join(
  tokens_clean,
  afinn,
  by = "word"
)

# Voir les scores
arranged_scores <- sentiment_scores %>%
  dplyr::select(word, value) %>%
  dplyr::arrange(dplyr::desc(value))
```

:::

::: {.column width="30%"}

```
r$> head(arranged_scores, 10)
       word value
1     super     3
2      good     3
3 excellent     3
4      good     3
5  generous     2
6 recommend     2
7     fresh     1
```

:::

::::

## Étape 6: Score total {transition="none"}

```r
total_sentiment <- dplyr::summarise(
  sentiment_scores,
  n_words = dplyr::n(),
  total_score = sum(value),
  avg_score = mean(value)
)

```

```
r$> total_sentiment
  n_words total_score avg_score
1       7          17  2.428571
```

## Avec plusieurs textes {.smaller transition="none"}

> Nothing exceptional, just edible. I had good feedback about the food and I was very, very disappointed. Not to mention cash only which for me is unacceptable. Too many good restaurants in the neighborhood, I won't go back there

> Food is good and price is ok. The only issu is the attitude of the staff. The lady at he cash register and the guy that takes the orders seriously lack client service skills. Both are  very rude. Hygiene is another issue, there are flies all over the place. In addition to all this, they only take cash.


```r
# Créer un data.frame avec plusieurs reviews
reviews <- data.frame(
  restaurant = "La ligne rouge",
  text = c(
    "Super good kebab! The portions are generous, the prices are really reasonable, and the quality is there. Tasty meat, fresh bread, and everything is well seasoned. An excellent address for a meal that is good without breaking the bank. I recommend!",
    "Nothing exceptional, just edible. I had good feedback about the food and I was very, very disappointed. Not to mention cash only which for me is unacceptable. Too many good restaurants in the neighborhood, I won't go back there",
    "Food is good and price is ok. The only issu is the attitude of the staff. The lady at he cash register and the guy that takes the orders seriously lack client service skills. Both are very rude. Hygiene is another issue, there are flies all over the place. In addition to all this, they only take cash."
  ),
  stringsAsFactors = FALSE
) %>%
  dplyr::mutate(id = 1:nrow(.))
```

## Nettoyage des multiples textes {transition="none"}


```r
reviews_clean <- reviews
reviews_clean$text <- stringr::str_to_lower(reviews_clean$text)                 # Minuscules
reviews_clean$text <- stringr::str_remove_all(reviews_clean$text, "!")          # Exclamations
reviews_clean$text <- stringr::str_remove_all(reviews_clean$text, "\\.")        # Points
reviews_clean$text <- stringr::str_remove_all(reviews_clean$text, ",")          # Virgules

```

## Étape 3: Tokenisation {transition="none"}


::::{.columns}

:::{.column width="60%"}

```r
tokens <- tidytext::unnest_tokens(
  reviews_clean,
  output = word,
  input = text
)

```
:::

:::{.column width="40%"}
```
r$> head(tokens, 10)
       restaurant id     word
1  La ligne rouge  1    super
2  La ligne rouge  1     good
3  La ligne rouge  1    kebab
4  La ligne rouge  1      the
5  La ligne rouge  1 portions
6  La ligne rouge  1      are
7  La ligne rouge  1 generous
8  La ligne rouge  1      the
9  La ligne rouge  1   prices
10 La ligne rouge  1      are
```
:::

::::

## Étape 4: Retrait des mots vides {transition="none"}


::::{.columns}

:::{.column width="60%"}

```r
# Obtenir les stop words
stop_words <- tidytext::get_stopwords(language = "en")

# Retirer les stop words avec dplyr anti_join
tokens_clean <- dplyr::anti_join(
  tokens, 
  stop_words,
  by = "word"
)

```
:::

:::{.column width="40%"}
```
r$> head(tokens_clean, 10)
       restaurant id       word
1  La ligne rouge  1      super
2  La ligne rouge  1       good
3  La ligne rouge  1      kebab
4  La ligne rouge  1   portions
5  La ligne rouge  1   generous
6  La ligne rouge  1     prices
7  La ligne rouge  1     really
8  La ligne rouge  1 reasonable
9  La ligne rouge  1    quality
10 La ligne rouge  1      tasty
```
:::

::::


## Étape 5: Analyse de sentiment (AFINN) {transition="none"}


::::{.columns}

:::{.column width="60%"}

```r
# Obtenir le lexique AFINN
afinn <- tidytext::get_sentiments("afinn")

# Joindre avec nos tokens
sentiment_scores <- dplyr::inner_join(
  tokens_clean,
  afinn,
  by = "word"
)

```

:::

:::{.column width="55%"}
```
r$> head(sentiment_scores, 10)
       restaurant id         word value
1  La ligne rouge  1        super     3
2  La ligne rouge  1         good     3
3  La ligne rouge  1     generous     2
4  La ligne rouge  1        fresh     1
5  La ligne rouge  1    excellent     3
6  La ligne rouge  1         good     3
7  La ligne rouge  1    recommend     2
8  La ligne rouge  2         good     3
9  La ligne rouge  2 disappointed    -2
10 La ligne rouge  2 unacceptable    -2
```
:::

::::

## Étape 6: Score total {.smaller transition="none"}


```r
# Calculate summary statistics per review
sentiment_summary <- sentiment_scores %>%
  group_by(id, restaurant) %>%
  summarise(
    total_sentiment = sum(value),            # Sum of all sentiment scores
    mean_sentiment = mean(value),            # Average sentiment
    word_count = n(),                        # Number of sentiment words
    min_sentiment = min(value),              # Most negative word
    max_sentiment = max(value)               # Most positive word
  ) %>%
  ungroup()


```
### Voici les résultats

```
r$> print(sentiment_summary)
# A tibble: 3 × 7
     id restaurant     total_sentiment mean_sentiment word_count min_sentiment max_sentiment
  <int> <chr>                    <dbl>          <dbl>      <int>         <dbl>         <dbl>
1     1 La ligne rouge              17           2.43          7             1             3
2     2 La ligne rouge               2           0.5           4            -2             3
3     3 La ligne rouge               1           0.5           2            -2             3
```


## Réassembler les données{.smaller transition="none"}

```r
# First, let's create a dataframe with just id and text
original_texts <- reviews %>%
  select(id, text)

# Then merge it with your sentiment summary
sentiment_summary_with_text <- sentiment_summary %>%
  left_join(original_texts, by = "id")
```
### En réunissant le text original avec les analyses:

```
r$> sentiment_summary_with_text
# A tibble: 3 × 8
     id restaurant     total_sentiment mean_sentiment word_count min_sentiment max_sentiment text                                                   
  <int> <chr>                    <dbl>          <dbl>      <int>         <dbl>         <dbl> <chr>                                                  
1     1 La ligne rouge              17           2.43          7             1             3 Super good kebab! [...]
2     2 La ligne rouge               2           0.5           4            -2             3 Nothing exceptional [...]
3     3 La ligne rouge               1           0.5           2            -2             3 Food is good and price is ok. [...]

```
## Lexicoder (LSD) {transition="none"}


- Spécialisé pour textes politiques
- Gestion avancée des négations
- Validation académique extensive
- Plus précis pour l'analyse politique

## Pourquoi LSD pour la science politique? {.smaller transition="none"}


1. **Contexte politique**
   - Vocabulaire politique spécifique
   - Expressions complexes
   - Nuances importantes

2. **Gestion des négations**
   - "not bad" ≠ "bad"
   - "no support" vs "support"

3. **Validation scientifique**
   - Testé sur des textes politiques
   - Utilisé dans des publications majeures
   - Benchmark en science politique

## Préparation des données pour LSD {transition="none"}


```r
library(quanteda)
library(pbapply)
library(dplyr)
library(LSX)

source("functions/lsd_prep_functions.R")

# Charger les reviews
reviews <- data.frame(
  restaurant = "La ligne rouge",
  text = c(
    "Super good kebab! The portions are generous, the prices are really reasonable, and the quality is there. Tasty meat, fresh bread, and everything is well seasoned. An excellent address for a meal that is good without breaking the bank. I recommend!",
    "Nothing exceptional, just edible. I had good feedback about the food and I was very, very disappointed. Not to mention cash only which for me is unacceptable. Too many good restaurants in the neighborhood, I won't go back there",
    "Food is good and price is ok. The only issu is the attitude of the staff. The lady at he cash register and the guy that takes the orders seriously lack client service skills. Both are very rude. Hygiene is another issue, there are flies all over the place. In addition to all this, they only take cash."
  ),
  stringsAsFactors = FALSE
) %>%
  dplyr::mutate(id = 1:nrow(.))

```

## Étape 2 : Division en phrases {.smaller transition="none"}

```r
# Diviser en phrases et créer ID unique
reviews_sentences <- reviews %>%
  tidytext::unnest_sentences(text, text) %>%
  mutate(id_sentence = row_number())

print(head(reviews_sentences, 2))
```
```
r$> head(reviews_sentences, 5)
      restaurant                                                                                   text id id_sentence
1 La ligne rouge                                                                      super good kebab!  1           1
2 La ligne rouge the portions are generous, the prices are really reasonable, and the quality is there.  1           2
3 La ligne rouge                              tasty meat, fresh bread, and everything is well seasoned.  1           3
4 La ligne rouge                an excellent address for a meal that is good without breaking the bank.  1           4
5 La ligne rouge                                                                           i recommend!  1           5
```

## Étape 3 : Application des fonctions de préparation {transition="none"}


```r
reviews_sentences$text_prepped <- NA
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text, LSDprep_contr)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, LSDprep_dict_punct)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, remove_punctuation_from_acronyms)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, remove_punctuation_from_abbreviations)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, LSDprep_punctspace)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, LSDprep_negation)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, LSDprep_dict)
reviews_sentences$text_prepped <- pbapply::pbsapply(reviews_sentences$text_prepped, mark_proper_nouns)
```

## Prétraitement LSD {.smaller transition="none"}


1. **Ordre des fonctions**
   - L'ordre d'application est important
   - Chaque fonction a un rôle spécifique

2. **Fonctions spéciales**
   - LSDprep_contr : Gère les contractions (don't → do not)
   - LSDprep_negation : Traite les négations
   - mark_proper_nouns : Identifie les noms propres

3. **Avantages**
   - Prétraitement standardisé
   - Meilleure gestion des négations
   - Plus précis pour l'analyse de sentiment


## Étape 4 : Création du corpus et analyse {transition="none"}



```r
# Créer le corpus avec les textes préparés
corpus_reviews <- corpus(
  reviews_sentences$text_prepped, 
  docnames = reviews_sentences$id_sentence
)

# Tokenisation
tokens_reviews <- tokens(
  corpus_reviews,
  what = "word",
  remove_punct = TRUE,
  remove_numbers = TRUE,
  remove_symbols = TRUE
)

# Conversion en minuscules
tokens_reviews <- tokens_tolower(tokens_reviews)
```

## Étape 5 : Création de la matrice document-terme {transition="none"}


```r
# Créer la matrice document-terme (DFM)
dfm_reviews <- dfm(tokens_reviews)

# Vérifier la structure
print(dfm_reviews)
```

## Analyse avec LSD {transition="none"}

```r
# Charger le dictionnaire LSD
data(data_dictionary_LSD2015)

# Appliquer le dictionnaire
sentiment_scores <- dfm_lookup(
  dfm_reviews, 
  dictionary = data_dictionary_LSD2015
)
```

```
r$> sentiment_scores
Document-feature matrix of: 15 documents, 4 features (76.67% sparse) and 0 docvars.
    features
docs negative positive neg_positive neg_negative
   1        0        2            0            0
   2        0        3            0            0
   3        0        2            0            0
   4        1        2            0            0
   5        0        0            0            0
   6        0        1            0            0
[ reached max_ndoc ... 9 more documents ]
```

## Interprétation des résultats LSD {transition="none"}

::::{.columns}

:::{.column width="60%"}
```r
# Créer un dataframe avec les scores
sentiment_df <- convert(sentiment_scores, to = "data.frame") %>%
  # Ajouter l'ID de l'article depuis reviews_sentences
  mutate(id = reviews_sentences$id)

# Agréger par article
lsd_metrics <- sentiment_df %>%
  group_by(id) %>%
  summarise(
    positif = sum(positive),
    negatif = sum(negative),
    score_net = positif - negatif,
    tone = (positif - negatif) / (positif + negatif)
  )

```
:::

:::{.column width="40%"}
```
r$> print(lsd_metrics)
# A tibble: 3 × 5
     id positif negatif score_net  tone
  <int>   <dbl>   <dbl>     <dbl> <dbl>
1     1       9       1         8   0.8
2     2       3       3         0   0  
3     3       2       2         0   0  
```
:::

::::

## Conseils pratiques

1. **Choisir le bon dictionnaire**
   - Textes politiques → LSD
   - Réseaux sociaux → AFINN/BING
   - Analyse émotionnelle → NRC

2. **Validation**
   - Vérifier manuellement les résultats
   - Comparer les dictionnaires
   - Documenter les choix méthodologiques

3. **Limites**
   - Aucun dictionnaire n'est parfait
   - Contexte est toujours important
   - Valider avec analyses qualitatives

## Quel lexique choisir? {.smaller}

| Lexique | Type de score | Forces | Utilisation idéale | Discipline |
|---------|---------------|---------|-------------------|------------|
| AFINN | -5 à +5 | Nuancé, simple | Médias sociaux, avis | Marketing |
| BING | Pos/Neg | Simple, précis | Analyses générales | Sciences sociales |
| NRC | 8 émotions | Riche en contexte | Analyse émotionnelle | Psychologie |
| Lexicoder | Binaire + thèmes | Validé académiquement | Discours politiques | Science politique |
| VADER | -1 à +1 | Gère emojis/web | Médias sociaux | Communications |## Exemple comparatif

## Lexicoder Sentiment Dictionary (LSD) {.smaller}



## Méthodes quantitatives pour analyse textuelle pas sans limites :no_entry_sign: {.smaller}

- Des idées sur des limites possibles?
- Nécessité d'analyser de gros corpus/volumes de textes. Les méthodes ne remplacent pas l'humain. Vous n'êtes pas capable de lire l'ensemble des textes tellement le corpus est volumineux... :anguished:
- Si vous pouvez lire le texte... **LISEZ-LE** et analysez-le avec des méthodes **QUALITATIVES**.
- Méthodes nécessitant d'importantes validations...

# Quiz 1
